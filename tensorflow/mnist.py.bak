# Needed for spark
spark = False
if spark:
    from pyspark.context import SparkContext
    from pyspark.conf import SparkConf

    sc = SparkContext(conf=SparkConf().setAppName("mnist_tripathi"))


def map_fun(worker_num):
    import tensorflow as tf

    class Train:
        def __init__(self):
            self.dropout_on = True

            # PlaceHolders
            self.x = tf.placeholder(tf.float32, shape=[None, 784])  # None X 784
            self.y_ = tf.placeholder(tf.float32, shape=[None, 10])
            self.keep_prob = tf.placeholder(tf.float32)

            #self.y_conv = self.forward_network()

            self.optimizer = tf.train.RMSPropOptimizer(1e-4, 0.99)

            self.x_buffer, self.y_conv_buffer, self.reward_buffer = [], [], []  # input states, actions taken, rewards



        @staticmethod
        def weight_variable(shape, name):
            initial = tf.truncated_normal(shape, stddev=0.1, name=name)
            return tf.Variable(initial)

        @staticmethod
        def bias_variable(shape, name):
            initial = tf.constant(0.1, shape=shape, name=name)
            return tf.Variable(initial)

        @staticmethod
        def conv2d(x, W, stride, name):
            return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME', name=name)

        @staticmethod
        def get_conv_layer(x_image, filter_size, input_depth, output_depth, stride, name):
            W = Train.weight_variable([filter_size, filter_size, input_depth, output_depth], name="W_" + name)
            b = Train.bias_variable([output_depth], name="B_" + name)

            return tf.nn.relu(Train.conv2d(x_image, W, stride=stride,
                                           name=name) + b)  # stride = 4, zero-padding - calculated automatically

        @staticmethod
        def get_fc_layer(x, input, output, name):
            W = Train.weight_variable([input, output], name="W_" + name)
            b = Train.bias_variable([output], name="B_" + name)
            return tf.nn.relu(tf.matmul(x, W) + b)

        def forward_network(self):

            x_image = tf.reshape(self.x, [-1, 28, 28, 1])  # None X 28 X 28 X 1
            h_conv1 = Train.get_conv_layer(x_image,
                                           filter_size=8,
                                           input_depth=1,
                                           output_depth=32,
                                           stride=4,
                                           name="conv1")

            h_conv2 = Train.get_conv_layer(h_conv1,
                                           filter_size=4,
                                           input_depth=32,
                                           output_depth=64,
                                           stride=2,
                                           name="conv2")

            h_conv3 = Train.get_conv_layer(h_conv2,
                                           filter_size=3,
                                           input_depth=64,
                                           output_depth=64,
                                           stride=1,
                                           name="conv3")

            # Fully connected layer 1
            h_pool2_flat = tf.reshape(h_conv3, [-1, 4 * 4 * 64])
            h_fc1 = Train.get_fc_layer(h_pool2_flat, input=4 * 4 * 64, output=1024, name="fc1");

            # Dropout
            h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob) if self.dropout_on else h_fc1

            # Fully connected layer 2
            h_fc2 = Train.get_fc_layer(h_fc1_drop, input=1024, output=10, name="fc2");

            # apply softmax to output
            return tf.nn.softmax(h_fc2)

        def apply_advantage_function(self):
            cross_entropy = tf.reduce_mean(-tf.reduce_sum(self.y_ * tf.log(y_conv), reduction_indices=[1]))


        def run(self):
            from tensorflow.examples.tutorials.mnist import input_data
            mnist = input_data.read_data_sets('mnist', one_hot=True)

            sess = tf.Session()

            y_conv = self.forward_network()

            cross_entropy = tf.reduce_mean(-tf.reduce_sum(self.y_ * tf.log(y_conv), reduction_indices=[1]))
            # gradients = tf.train.RMSPropOptimizer(1e-4, 0.99).compute_gradients(cross_entropy)

            train_step = tf.train.RMSPropOptimizer(1e-4, 0.99).minimize(cross_entropy)
            correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(self.y_, 1))
            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
            sess.run(tf.initialize_all_variables())
            for i in range(20000):
                batch = mnist.train.next_batch(50)
                if i % 100 == 0:
                    train_accuracy = accuracy.eval(session=sess,
                                                   feed_dict={self.x: batch[0], self.y_: batch[1], self.keep_prob: 1.0})
                    print("step %d, training accuracy %g" % (i, train_accuracy))
                train_step.run(session=sess, feed_dict={self.x: batch[0], self.y_: batch[1], self.keep_prob: 0.5})

            print("test accuracy %g" % accuracy.eval(session=sess, feed_dict={
                self.x: mnist.test.images, self.y_: mnist.test.labels, self.keep_prob: 1.0}))

    train = Train()
    train.run()


if spark:
    numExecutors = int(sc._conf.get("spark.executor.instances"))
    rdd = sc.parallelize(range(numExecutors), numExecutors)
    rdd.map(map_fun).collect()

map_fun(1)
